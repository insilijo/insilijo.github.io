<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://insilijo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://insilijo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-12T17:12:18+00:00</updated><id>https://insilijo.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://insilijo.github.io/blog/2026/2026-01-11-ra-cohorts-graphs/" rel="alternate" type="text/html" title=""/><published>2026-01-12T17:12:18+00:00</published><updated>2026-01-12T17:12:18+00:00</updated><id>https://insilijo.github.io/blog/2026/2026-01-11-ra-cohorts-graphs</id><content type="html" xml:base="https://insilijo.github.io/blog/2026/2026-01-11-ra-cohorts-graphs/"><![CDATA[<p>I started <strong>ra_cohorts</strong> to make multi-cohort validation honest. The part I return to most often is the <strong>grid search</strong> used inside nested cross-validation. It looks like a mundane engineering step, but it encodes the math choices I care about: what loss to optimize, how to regularize, and how to control bias when cohorts shift.</p> <h2 id="the-core-objective">The core objective</h2> <p>For binary outcomes, the pipeline optimizes AUC on held-out folds. For a model with parameters θ, the objective is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>maximize  AUC( f(x; θ), y )
</code></pre></div></div> <p>This matters because AUC is insensitive to class imbalance and ties directly to ranking quality. In <code class="language-plaintext highlighter-rouge">scripts/validation_pipeline.py</code>, the outer loop measures generalization, while the inner loop selects hyperparameters that maximize AUC.</p> <h2 id="grid-search-as-explicit-regularization">Grid search as explicit regularization</h2> <p>The grid is small by design. It encodes prior beliefs about the model class and its capacity:</p> <ul> <li><strong>SVC (RBF kernel):</strong> C in {0.01, 0.1, 1, 10}, gamma in {scale, auto}</li> <li><strong>Logistic regression:</strong> C in {0.01, 0.1, 1, 10}</li> </ul> <p>For SVMs, C is the inverse of the regularization strength. In a soft-margin formulation:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>minimize  1/2 ||w||^2 + C * Σ ξ_i
</code></pre></div></div> <p>Small C biases toward wider margins (higher bias, lower variance), large C allows tighter fit (lower bias, higher variance). The grid is a controlled exploration of this bias-variance trade.</p> <p>For logistic regression, C plays the same role for L2 regularization:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>minimize  Σ log(1 + exp(-y_i w^T x_i)) + (1/2C) ||w||^2
</code></pre></div></div> <p>The grid is a statement: I am not searching the space of models, I am searching the space of <strong>regularization regimes</strong>.</p> <h2 id="nested-cv-to-avoid-leakage">Nested CV to avoid leakage</h2> <p>The pipeline uses nested CV so that hyperparameter selection is isolated from the final evaluation. The flow is:</p> <ol> <li>Split training cohorts into outer folds.</li> <li>For each outer fold, run inner CV to select (C, gamma).</li> <li>Evaluate on the outer fold.</li> <li>Train on all training cohorts using the selected configuration, then test on the held-out cohort.</li> </ol> <p>This matters because without the inner loop, hyperparameter selection would leak information from the test fold and inflate performance.</p> <h2 id="when-graphs-enter">When graphs enter</h2> <p>The same logic shows up in the WL-kernel path. Graphs are mapped into a kernel matrix K, and the classifier is trained with a precomputed kernel:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>K_ij = k(G_i, G_j)
</code></pre></div></div> <p>Here, the grid is still on C (and not the kernel hyperparameters), which keeps the experiment focused: <em>How sensitive is the classifier to regularization when the similarity function is fixed?</em></p> <h2 id="why-this-is-the-right-abstraction">Why this is the right abstraction</h2> <p>In multi-cohort work, the biggest risk is tuning to cohort idiosyncrasies. A tight grid inside nested CV is the simplest way to formalize humility:</p> <ul> <li>keep the model class small</li> <li>evaluate capacity explicitly</li> <li>prevent selection bias</li> </ul> <p>That is the math behind the grid search in <strong>ra_cohorts</strong>. It is not just a convenience. It is the only reliable way I have found to keep performance claims grounded when cohorts disagree.</p>]]></content><author><name></name></author></entry><entry><title type="html">Scientific Babylon</title><link href="https://insilijo.github.io/blog/2026/scientific-babylon/" rel="alternate" type="text/html" title="Scientific Babylon"/><published>2026-01-10T12:00:00+00:00</published><updated>2026-01-10T12:00:00+00:00</updated><id>https://insilijo.github.io/blog/2026/scientific-babylon</id><content type="html" xml:base="https://insilijo.github.io/blog/2026/scientific-babylon/"><![CDATA[<p>In accounts from The Bible and The Torah, the Tower of Babel is built to prevent a second flood, only to result in the fragmentation of human language. Variations on the same theme appear across Greek, Estonian, Sumerian, and Aztec traditions: a once-unified humanity loses a shared understanding of the world through linguistic division. In these stories, collective power gives way to confusion: not through catastrophe, but through meaning itself.</p> <figure class="inline-figure inline-right"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/2026-01-01/tower_of_babylon-480.webp 480w,/assets/img/blogs/2026-01-01/tower_of_babylon-800.webp 800w,/assets/img/blogs/2026-01-01/tower_of_babylon-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blogs/2026-01-01/tower_of_babylon.jpg" width="100%" height="auto" alt="The Tower of Babel By Pieter Brueghel the Elder - Levels adjusted from File:Pieter_Bruegel_the_Elder_-_The_Tower_of_Babel_(Vienna)_-_Google_Art_Project.jpg, originally from Google Art Project., Public Domain, https://commons.wikimedia.org/w/index.php?curid=22179117" title="The Tower of Babel By Pieter Brueghel the Elder - Levels adjusted from File:Pieter_Bruegel_the_Elder_-_The_Tower_of_Babel_(Vienna)_-_Google_Art_Project.jpg, originally from Google Art Project., Public Domain, https://commons.wikimedia.org/w/index.php?curid=22179117" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption small">The Tower of Babel By Pieter Brueghel the Elder - Levels adjusted from File:Pieter_Bruegel_the_Elder_-_The_Tower_of_Babel_(Vienna)_-_Google_Art_Project.jpg, originally from Google Art Project., Public Domain, https://commons.wikimedia.org/w/index.php?curid=22179117</figcaption> </figure> <p>Modern linguistics offers a less mythic explanation. Languages diverge naturally, shaped by ecology, geography, isolation, and social structure. Language is not merely a labeling system; it is a lens for interpreting a complex world. As perspectives diverge, so too must the structures used to describe them.</p> <p>Science faces a related, but sharper, problem. We attempt to describe systems that are not only complex, but mostly unobservable, probabilistic, and dynamic. The challenge extends past measurement and into representation: data are meaningless without a structured, shared, and preserved system. That system has to subscribe to the same orders as all data: Findable, Accessible, Interoperable, and Reproducible (FAIR) because its value lies in how it interfaces with colleagues and other data.</p> <p>Ontologies attempt to resolve this fragmentation by enforcing shared meaning. Here, ontology is used in its applied sense: not as a claim about what exists, but as a practical expression of how knowledge is organized and exchanged. In practice, they often expose the cost of assuming meaning can be fixed at all. Applying an alternative standard to these processes – instead of resulting in meaning – adds another standard on top of the dozens already there. We confront our own Tower of Babel, then: we find ourselves playing a massive, expensive game of telephone where meaning is exchanged, lost, and mutated between experts.</p> <p>In metabolomics alone, biological, chemical, and analytical vocabularies coexist; each developed in different ecologies, each optimized for a different audience, and each only partially compatible with the others.</p> <p>These ontologies operate <strong>long before</strong> interpretation is even possible. Moreover, each individual discipline involved is nuanced, requiring years of experience or education to get to foundational understanding. These ontologies are the connective tissue that flattens these disciplinary nuances. For example, metabolomics data flows from Liquid Chromatography/Mass Spectrometry (LC/MS) through reference spectra (ideally developed on the same machine and method) and finally to a format that’s accessible to a biochemist (typically a table). Even in this simplified process, it’s clear that there’s a substantial amount of effort involved in developing the method, creating the infrastructure around managing/storing the data, selecting/synthesizing reference compounds, analyzing data, and engineering it for interpretation.</p> <p>Even getting to the point where we’re comfortable analyzing the data requires a tremendous amount of effort and coordination. Moreover, no scientist has the capacity or time to ensure quality of the product, and relies on the data originator to deliver precise, accurate, and relevant results. It’s crucial, for both internal and external purposes, to maintain a paper trail that makes it clear how each step integrates into the useful data. Otherwise, we’re left with a mess of isolated numbers.</p> <figure class="inline-figure inline-left xkcd-figure"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blogs/2026-01-01/xkcd_927_standards-480.webp 480w,/assets/img/blogs/2026-01-01/xkcd_927_standards-800.webp 800w,/assets/img/blogs/2026-01-01/xkcd_927_standards-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blogs/2026-01-01/xkcd_927_standards.png" width="320" height="auto" alt="xkcd 927 Standards comic" title="xkcd 927 Standards comic" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption small">xkcd 927 &quot;Standards&quot; by Randall Munroe, CC BY-NC 2.5, https://xkcd.com/927/</figcaption> </figure> <p>This isn’t just a factor in metabolomics. In fact, this specific analytical example can be applied directly to proteomics with a few small tweaks. Genomics, too, can claim to have done an excellent job of exploiting integrated analytics and ontologies to create reliable, consistent pipelines that are broadly interpretable and trustworthy. What we’re trying to do here – use extremely expensive, sensitive instruments on extremely expensive, sensitive biological samples – is difficult and important. The first step is being able to adequately describe what’s going on to other people.</p> <p>Ontologies are, at their most useful, a representation of shared purpose among diverse methods, all integrating into a cohesive representation of an intractable phenomenon. To get to that point, it requires often silent, detailed labor from a large group of people to map out and maintain a reliable pipeline. However, they can often fall prone to overly-calcified standards, resulting in a labyrinthine branched set of systems. These processes must reflect their application while remaining integrated with their partners. To do this, ontologies are most effective when they constrain interpretation without attempting to freeze meaning. In domains that evolve as quickly as medicine, chemistry, tech, and biology, representation must remain thoughtfully provisional without becoming unstable.</p> <p>What looks like a problem of standards is often a problem of governance. The failure mode here is rarely technical. It is organizational. Different groups optimize for different incentives, audiences, or realities – speed, precision, publication, novelty, regulatory defensibility, commercial relevance – and ontologies become the battleground where those incentives collide. Standardization does not remove ambiguity; it decides who bears the cost of resolving it.</p> <p>In practice, governance does not mean tighter standards or different hiring practices. It means deciding who can revise definitions, how translation is handled, and where ambiguity is tolerated. Meaning will change regardless; the real question is who is accountable for managing that change over time.</p>]]></content><author><name></name></author><category term="software"/><category term="science"/><category term="ontologies"/><category term="standards"/><category term="omics"/><category term="multiomics"/><summary type="html"><![CDATA[An essay on how meaning, incentives, and governance shape scientific systems across teams and institutions.]]></summary></entry></feed>