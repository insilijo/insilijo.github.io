---
layout: post
title: When the levees break
date: 2026-02-15 12:00:00
description: AI without containment guarantees systemic cost reallocation and markets will eventually correct
tags: AI integration structure boundaries science integration
categories: organizations science ai
giscus_comments: true
related_posts: true
pretty_table: true
thumbnail: "assets/img/blogs/2026-02-15/olympics_opening_ceremony.webp"
---

{% include figure.liquid
  loading="eager"
  path="assets/img/blogs/2026-02-15/olympics_opening_ceremony.webp"
  caption="Still from Milano Cortina Winter Olympics 2026, NBC/Peacock."
  figure_class="inline-figure inline-left"
  caption_class="caption small"
%}

The 2026 Winter Olympics in Milan opened with AI-assisted imagery. The Superbowl featured 15 AI-centered commercials: nearly \\$120 million in ad spend. This has led [some analysts like Carl Brown and the folks he cites](https://www.youtube.com/watch?v=Z68ncMsEgsI) to note the eerie similarities to the 2000 dot-com bubble where 14 of 61 spots were dot-com related. At \\$2.2 million per spot, that's \\$30.8 million, or nearly \\$60 million in today's dollars.

The word now circulating is “bubble.” The comparison is obvious: it looks increasingly like a high-risk game of hot potato, played between 7 companies. Bloomberg’s Odd Lots [recently suggested](https://www.youtube.com/watch?v=z4ct_eDYx2A) this cycle combines dot-com excess, real estate leverage, and aggressive financing models. We're seeing massive investments in a technology with real applications but without the promised productivity boon. 

AI will almost certainly endure. It compresses cognition the way the internet compressed distribution. The capability expansion is real: the question is not whether it persists, but how institutions metabolize it.

{% include video.liquid
path="assets/video/circular_data_economy.mp4"
controls=true
caption="Projected money flow between major AI companies by user jcceagle on [reddit.com/r/dataisbeautiful](https://www.reddit.com/r/dataisbeautiful/comments/1ppla7o/oc_mapping_the_flow_of_revenue_and_investment/) using PlotSet" %}

### Dissolution of self
The public reaction hasn’t been awe. It’s been ridicule. Instead of being wowed by the technology, the discourse is tearing it apart: on the world's largest stages, we've seen unconscionable brand mistakes like incorrect logos, gibberish words, and eerie *I, Robot* style images.

What I think the public is responding to is not the innate benefit or limitation of the technology; that story is yet to be written. It's the misalignment of the technology with reality. AI is a boundary-eroding force that redistributes cost, responsibility, and meaning faster than institutions can re-contain them. We're operating at the gap between promise and realization.

{% include figure.liquid
  loading="eager"
  path="assets/img/blogs/2026-02-15/tasks.png"
  caption="In CS, it can be hard to explain the difference between the easy and the virtually impossible. xkcd.com/1425, *Tasks*, by Randall Munroe"
  figure_class="inline-figure inline-right"
  caption_class="caption small"
%}

From my previous post, I argue that a common institutional conceptualization of boundaries is core to mission focus and execution. Our existence in AI limbo indicates we're in an unbounded, unrealized territory. Institutions rely on boundaries to assign cost, responsibility, and deferral. When those boundaries fail, valuation detaches from structural reliability. Perceived value detaches when execution lags expectation.

In the dot-com bubble, we saw a new technology (that has since become fundamental!) that excluded revenue models and profitability. What we gained was a completely novel industrial sector and productivity gains, but only after a devastating market crash that compromised the whole technological sector. Valuation outran integration and then integration became its own viable industry.

In the dot-com era, integration became its own industry — DevOps, cloud infrastructure, cybersecurity. AI will follow a similar path. The winners will build internal AI governance and review systems as products, not policies.

As before, so now: gibberish, the hallucinations, the energy expenditure, these are potentially just engineering defects. Engineering defects get solved. But the defects are not the real signal: what is harder to solve is boundary dissolution.

AI collapses distinctions that institutions depend on:
* Brand vs. Designer — Who is responsible when the logo is wrong?
* Author vs. Model — Who owns the sentence and its meaning?
* Expert vs. Prompt Engineer — Who is accountable for the analysis and its communication?
* Junior vs. Senior — Who absorbs review cost and ensures architectural compatibility?
* Tool vs. Decision-Maker — Who signs off and under what conditions?
* Labor vs. Capital — Who captures productivity gains?

These distinctions are not philosophical. They are how we assign cost, accountability, and compensation. When AI compresses them, output accelerates but responsibility diffuses. And diffusion feels like efficiency until the bill arrives. Or until you have to do something with all that output.

### There is no singular AI economy
AI commands a broad, infrastructural change, but what both proponents and detractors identify is not what it does on the macroscopic level, but what happens on the individual level. Whether people argue that it extends an individual's faculty or replaces them entirely, the decisions that AI enables are primarily scaleable microscopic decisions. A backend engineer can rapidly prototype a front-end that they can pass on to their peer. A hobbyist trader can pull data and apply machine learning algorithms to predict the right time to be stocks consistent with non-proprietary quantitative traders.

Individually, none of this destabilizes. At scale, it compounds. Each AI-assisted decision externalizes cost to infrastructure, oversight, energy, liability. Individually trivial. Systemically compounding.

There is no singular “AI economy.” There are millions of boundary crossings embedded in marketing teams, codebases, research labs, legal drafts, and art studios. Each reduces local friction while exporting integration cost elsewhere.

Systems can absorb small deferrals. At scale, those deferrals synchronize. Infrastructure costs rise. Trust erodes. Accountability diffuses. Capital reallocates before integration catches up. Eventually, no one fully understands the system they depend on. Output accumulates faster than integration capacity.

The durable institutions will not eliminate boundaries; they will redesign them. Instead of “AI replaces junior engineers,” they will define new review layers, new authorship standards, new liability checkpoints. Boundary collapse is destabilizing. Boundary redesign is competitive advantage.

AI is boundary arbitrage at scale. It allows individuals to capture upside while exporting integration cost into shared infrastructure.

### The paradox of governance
The sanitized executive narrative is empowerment: higher productivity per employee. The unsanitized version is compression: fewer employees per function.

AI lowers the cost of crossing boundaries. But institutions were built around those boundaries. When crossing becomes cheap, containment becomes expensive. Oversight, review, and coordination were not inefficiencies. They were containment infrastructure.

AI does not integrate itself. It requires deliberate boundary redesign, cross-functional communication, and explicit ownership of review, infrastructure, and liability cost. Without that, what looks like productivity is simply deferred integration expense.

AI without explicit integration design is not transformation. It is cost redistribution.

While AI promises access to upskilling, it's worth contending with Chesterton's Fence here. Responsibilities are encoded institutional memory; they are not just skill, they are context. In some cases it's useful to break and revisit these systems, but in others it can be catastrophic. We come by these containment structures honestly; they were direct responses to gaps between technical capability. 

When disrupted, context bleeds out first. Authorship diffuses, labor is conducted elsewhere, and liability is distributed beyond the person best equipped to handle it. Previously created pathways effecting formal oversight, audit layers, internal controls, and legal containment are now broken because the technical function is now elsewhere.

AI creates the appearance of productivity gains by compressing labor boundaries. In exchange, it increases governance volatility. This is not a tooling problem. It is an accounting problem, a liability problem, and a governance problem. Institutions that do not redesign their containment structures will discover that efficiency gains were cost deferrals.

### Why bubbles form
A bubble forms when expectation outpaces containment.

The internet ultimately transformed the economy, but only after valuation collapsed to match integration capacity. Infrastructure, tooling, governance, and discipline lagged promise.

The internet endured. Valuation did not.

The mistake was temporal. Valuation preceded reintegration. The technology and infrastructure were disruptive but immature, leading to a boundary erosion of existing systems but a lack of reintegration into the new world.

{% include figure.liquid
  loading="eager"
  path="assets/img/blogs/2026-02-15/stages_of_a_bubble.webp"
  caption="Stages in a Bubble. Dr. Jean-Paul Rodrigue, Dept. of Global Studies & Geography, Hofstra University."
  caption_class="caption small"
%}

AI is facing a similar climate. We're in a race to support the technology, but it's unclear how we do so at scale. Institutional absorption is immature and the displacements and deferrals are not yet priced-in or addressed. Cost reallocation is opaque. Governance and regulation are underdeveloped. Labor displacement is unpriced. Brand and liability containment are unclear.

The technology has disrupted core economic and technical infrastructure. Crucial variables are now unclaimed. The membrane of the system is broken: responsibility is unassigned, liability is unpriced, and labor is displaced faster than governance can reconstitute it. For an economy that prioritizes capital flow and certainty, those deferrals must be addressed.

### Who wins when the bubble pops?
Losses follow when integration lags valuation. Winners invest in reintegration before markets force them to. They price governance as infrastructure and don't overindex the early hyped applications and spectacles. They treat workforce integration as asset development. They use AI for system augmentation, not output acceleration.

{% include figure.liquid
  loading="eager"
  path="assets/img/blogs/2026-02-15/ai_slip_opening_ceremony.webp"
  caption="Still from Milano Cortina Winter Olympics 2026, NBC/Peacock."
  figure_class="inline-figure inline-right"
  caption_class="caption small"
%}

Reintegration requires explicit cost attribution. If AI reduces labor in one function but increases review, infrastructure, and liability exposure elsewhere, those costs must be surfaced rather than absorbed invisibly. Organizations that treat AI as net labor compression without integrated cost accounting will misprice their gains.

The winners will view the potentially displaced workers as ones that the technology will invest in and improve, not displace. The winners will not be those who maximize output. They will be those who intentionally manage and internalize displacement before the market forces them to.

A bubble is not excessive investment. It is enthusiasm expanding faster than containment.

That's where we are: AI appears mature because it is visible, but visibility is not integration. It's spectacle. Until then, we'll see JECK OUTIbIyMDES D'AIIVER at the Milan Winter Olympics.

Markets price acceleration. Reality prices containment.