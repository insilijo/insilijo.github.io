<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>I started <strong>ra_cohorts</strong> to make multi-cohort validation honest. The part I return to most often is the <strong>grid search</strong> used inside nested cross-validation. It looks like a mundane engineering step, but it encodes the math choices I care about: what loss to optimize, how to regularize, and how to control bias when cohorts shift.</p> <h2 id="the-core-objective">The core objective</h2> <p>For binary outcomes, the pipeline optimizes AUC on held-out folds. For a model with parameters θ, the objective is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>maximize  AUC( f(x; θ), y )
</code></pre></div></div> <p>This matters because AUC is insensitive to class imbalance and ties directly to ranking quality. In <code class="language-plaintext highlighter-rouge">scripts/validation_pipeline.py</code>, the outer loop measures generalization, while the inner loop selects hyperparameters that maximize AUC.</p> <h2 id="grid-search-as-explicit-regularization">Grid search as explicit regularization</h2> <p>The grid is small by design. It encodes prior beliefs about the model class and its capacity:</p> <ul> <li> <strong>SVC (RBF kernel):</strong> C in {0.01, 0.1, 1, 10}, gamma in {scale, auto}</li> <li> <strong>Logistic regression:</strong> C in {0.01, 0.1, 1, 10}</li> </ul> <p>For SVMs, C is the inverse of the regularization strength. In a soft-margin formulation:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>minimize  1/2 ||w||^2 + C * Σ ξ_i
</code></pre></div></div> <p>Small C biases toward wider margins (higher bias, lower variance), large C allows tighter fit (lower bias, higher variance). The grid is a controlled exploration of this bias-variance trade.</p> <p>For logistic regression, C plays the same role for L2 regularization:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>minimize  Σ log(1 + exp(-y_i w^T x_i)) + (1/2C) ||w||^2
</code></pre></div></div> <p>The grid is a statement: I am not searching the space of models, I am searching the space of <strong>regularization regimes</strong>.</p> <h2 id="nested-cv-to-avoid-leakage">Nested CV to avoid leakage</h2> <p>The pipeline uses nested CV so that hyperparameter selection is isolated from the final evaluation. The flow is:</p> <ol> <li>Split training cohorts into outer folds.</li> <li>For each outer fold, run inner CV to select (C, gamma).</li> <li>Evaluate on the outer fold.</li> <li>Train on all training cohorts using the selected configuration, then test on the held-out cohort.</li> </ol> <p>This matters because without the inner loop, hyperparameter selection would leak information from the test fold and inflate performance.</p> <h2 id="when-graphs-enter">When graphs enter</h2> <p>The same logic shows up in the WL-kernel path. Graphs are mapped into a kernel matrix K, and the classifier is trained with a precomputed kernel:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>K_ij = k(G_i, G_j)
</code></pre></div></div> <p>Here, the grid is still on C (and not the kernel hyperparameters), which keeps the experiment focused: <em>How sensitive is the classifier to regularization when the similarity function is fixed?</em></p> <h2 id="why-this-is-the-right-abstraction">Why this is the right abstraction</h2> <p>In multi-cohort work, the biggest risk is tuning to cohort idiosyncrasies. A tight grid inside nested CV is the simplest way to formalize humility:</p> <ul> <li>keep the model class small</li> <li>evaluate capacity explicitly</li> <li>prevent selection bias</li> </ul> <p>That is the math behind the grid search in <strong>ra_cohorts</strong>. It is not just a convenience. It is the only reliable way I have found to keep performance claims grounded when cohorts disagree.</p> </body></html>